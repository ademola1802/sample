{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55db2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851d7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 20:58:04.963485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 20:58:05.530903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras.layers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0349b24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>event</th>\n",
       "      <th>sub_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hurricane harvey 2017 forecast  more strengthe...</td>\n",
       "      <td>information dissemination</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why do they name hurricanes  i feel like peopl...</td>\n",
       "      <td>inquiry</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has been bashed again for nt helping nepal wo...</td>\n",
       "      <td>information dissemination</td>\n",
       "      <td>emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harvey would be 64th hurricane to hit texas si...</td>\n",
       "      <td>information dissemination</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>live  us president  arrives in atlantic ocean ...</td>\n",
       "      <td>information dissemination</td>\n",
       "      <td>inspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38091</th>\n",
       "      <td>the selena statue is braced for hurricane har...</td>\n",
       "      <td>safety</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38092</th>\n",
       "      <td>tnn    capital of nepal earthquake leaves bui...</td>\n",
       "      <td>casualty</td>\n",
       "      <td>intensity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38093</th>\n",
       "      <td>stay safe out there   \\n  hurricaneharvey</td>\n",
       "      <td>safety</td>\n",
       "      <td>well wishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38094</th>\n",
       "      <td>armenia  vanadzor nepal quake kills 449  trig...</td>\n",
       "      <td>die</td>\n",
       "      <td>intensity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38095</th>\n",
       "      <td>single valued function out of camp down on eve...</td>\n",
       "      <td>information dissemination</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38096 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      hurricane harvey 2017 forecast  more strengthe...   \n",
       "1      why do they name hurricanes  i feel like peopl...   \n",
       "2       has been bashed again for nt helping nepal wo...   \n",
       "3      harvey would be 64th hurricane to hit texas si...   \n",
       "4      live  us president  arrives in atlantic ocean ...   \n",
       "...                                                  ...   \n",
       "38091   the selena statue is braced for hurricane har...   \n",
       "38092   tnn    capital of nepal earthquake leaves bui...   \n",
       "38093          stay safe out there   \\n  hurricaneharvey   \n",
       "38094   armenia  vanadzor nepal quake kills 449  trig...   \n",
       "38095  single valued function out of camp down on eve...   \n",
       "\n",
       "                           event    sub_event  \n",
       "0      information dissemination       update  \n",
       "1                        inquiry     question  \n",
       "2      information dissemination      emotion  \n",
       "3      information dissemination       update  \n",
       "4      information dissemination   inspection  \n",
       "...                          ...          ...  \n",
       "38091                     safety       update  \n",
       "38092                   casualty    intensity  \n",
       "38093                     safety  well wishes  \n",
       "38094                        die    intensity  \n",
       "38095  information dissemination     location  \n",
       "\n",
       "[38096 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"/home/aaadfg/Downloads/disaster_paper/data/final_data/train2.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23f7faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>event</th>\n",
       "      <th>sub_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why hurricane harvey could be a huge problem f...</td>\n",
       "      <td>disaster preparedness</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest hurricane hunter flight of stairs indic...</td>\n",
       "      <td>climate and environmental issues</td>\n",
       "      <td>intensity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shoutout to  for being in that location for us...</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>relief assistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hurricane harvey strengthens to category 2 sto...</td>\n",
       "      <td>safety</td>\n",
       "      <td>intensity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hurricane harvey threatens travel along texas ...</td>\n",
       "      <td>safety</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9520</th>\n",
       "      <td>everest climbers in camp 1 survived but route...</td>\n",
       "      <td>safety</td>\n",
       "      <td>rescue support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9521</th>\n",
       "      <td>this  hurricaneharvey monish but avail deoxyad...</td>\n",
       "      <td>warning</td>\n",
       "      <td>comparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9522</th>\n",
       "      <td>hurricane harvey headed for area with signific...</td>\n",
       "      <td>information dissemination</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9523</th>\n",
       "      <td>700 000 darling died indium hurricane katrindi...</td>\n",
       "      <td>casualty</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9524</th>\n",
       "      <td>this is brock long  the current director of   ...</td>\n",
       "      <td>empathy</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9525 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0     why hurricane harvey could be a huge problem f...   \n",
       "1     latest hurricane hunter flight of stairs indic...   \n",
       "2     shoutout to  for being in that location for us...   \n",
       "3     hurricane harvey strengthens to category 2 sto...   \n",
       "4     hurricane harvey threatens travel along texas ...   \n",
       "...                                                 ...   \n",
       "9520   everest climbers in camp 1 survived but route...   \n",
       "9521  this  hurricaneharvey monish but avail deoxyad...   \n",
       "9522  hurricane harvey headed for area with signific...   \n",
       "9523  700 000 darling died indium hurricane katrindi...   \n",
       "9524  this is brock long  the current director of   ...   \n",
       "\n",
       "                                 event          sub_event  \n",
       "0                disaster preparedness             update  \n",
       "1     climate and environmental issues          intensity  \n",
       "2                         appreciation  relief assistance  \n",
       "3                               safety          intensity  \n",
       "4                               safety             update  \n",
       "...                                ...                ...  \n",
       "9520                            safety     rescue support  \n",
       "9521                           warning         comparison  \n",
       "9522         information dissemination             update  \n",
       "9523                          casualty             update  \n",
       "9524                           empathy             update  \n",
       "\n",
       "[9525 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"/home/aaadfg/Downloads/disaster_paper/data/final_data/test2.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85529bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>event</th>\n",
       "      <th>sub_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hurricane harvey 2017 forecast  more strengthe...</td>\n",
       "      <td>15</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why do they name hurricanes  i feel like peopl...</td>\n",
       "      <td>16</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has been bashed again for nt helping nepal wo...</td>\n",
       "      <td>15</td>\n",
       "      <td>emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harvey would be 64th hurricane to hit texas si...</td>\n",
       "      <td>15</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>live  us president  arrives in atlantic ocean ...</td>\n",
       "      <td>15</td>\n",
       "      <td>inspection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  event   sub_event\n",
       "0  hurricane harvey 2017 forecast  more strengthe...     15      update\n",
       "1  why do they name hurricanes  i feel like peopl...     16    question\n",
       "2   has been bashed again for nt helping nepal wo...     15     emotion\n",
       "3  harvey would be 64th hurricane to hit texas si...     15      update\n",
       "4  live  us president  arrives in atlantic ocean ...     15  inspection"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually re-number the dictionary values\n",
    "encoded_dict = {\n",
    "    \"admiration\": 1,\n",
    "    \"appreciation\": 2,\n",
    "    \"business\": 3,\n",
    "    \"casualty\": 4,\n",
    "    \"climate and environmental issues\": 5,\n",
    "    \"communication\": 6,\n",
    "    \"damage\": 7,\n",
    "    \"die\": 8,\n",
    "    \"disaster preparedness\": 9,\n",
    "    \"education\": 10,\n",
    "    \"empathy\": 11,\n",
    "    \"health\": 12,\n",
    "    \"humanitarian assistance\": 13,\n",
    "    \"immigration\": 14,\n",
    "    \"information dissemination\": 15,\n",
    "    \"inquiry\": 16,\n",
    "    \"life\": 17,\n",
    "    \"memories\": 18,\n",
    "    \"news\": 19,\n",
    "    \"others\": 20,\n",
    "    \"personal matters\": 21,\n",
    "    \"politics\": 22,\n",
    "    \"resources\": 23,\n",
    "    \"safety\": 24,\n",
    "    \"sport\": 25,\n",
    "    \"spiritual\": 26,\n",
    "    \"transportation\": 27,\n",
    "    \"travel\": 28,\n",
    "    \"warning\": 29\n",
    "}\n",
    "\n",
    "# Update the 'event' column in the DataFrame\n",
    "df_train['event'] = df_train.event.map(encoded_dict)\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b003bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>event</th>\n",
       "      <th>sub_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why hurricane harvey could be a huge problem f...</td>\n",
       "      <td>9</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest hurricane hunter flight of stairs indic...</td>\n",
       "      <td>5</td>\n",
       "      <td>intensity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shoutout to  for being in that location for us...</td>\n",
       "      <td>2</td>\n",
       "      <td>relief assistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hurricane harvey strengthens to category 2 sto...</td>\n",
       "      <td>24</td>\n",
       "      <td>intensity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hurricane harvey threatens travel along texas ...</td>\n",
       "      <td>24</td>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  event          sub_event\n",
       "0  why hurricane harvey could be a huge problem f...      9             update\n",
       "1  latest hurricane hunter flight of stairs indic...      5          intensity\n",
       "2  shoutout to  for being in that location for us...      2  relief assistance\n",
       "3  hurricane harvey strengthens to category 2 sto...     24          intensity\n",
       "4  hurricane harvey threatens travel along texas ...     24             update"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['event'] = df_test.event.map(encoded_dict)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e302c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda3a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da9b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(df_train.event)\n",
    "y_test = to_categorical(df_test.event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c56128f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c138485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ae938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer,TFBertModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# bert = TFBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d61982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer,TFBertModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "674d3c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 20:59:48.877691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:48.952350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:48.952501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:48.954191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:48.954308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:48.954405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:49.014355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:49.014478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:49.014577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 20:59:49.014662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22082 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2023-09-18 20:59:49.298442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFRobertaModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "bert = TFRobertaModel.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff59b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, TFXLNetModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
    "# bert = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ad7398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input (takes some time)\n",
    "# here tokenizer using from bert-base-cased\n",
    "x_train = tokenizer(\n",
    "    text=df_train.tweet.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=43,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "x_test = tokenizer(\n",
    "    text=df_test.tweet.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=43,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0edf46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x_train['input_ids']\n",
    "attention_mask = x_train['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78727d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da9aca2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 43\n",
    "shape=(max_len,)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a382370",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 43\n",
    "shape=(max_len,)\n",
    "shape\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[0]\n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "# y = Dense(49,activation = 'sigmoid')(out)\n",
    "# model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "\n",
    "y = Dense(30, activation='softmax')(out)  \n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "\n",
    "model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d06774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=5e-05,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0)\n",
    "\n",
    "# optimizer = Adam(learning_rate=lr_schedule, epsilon=1e-08, clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss =CategoricalCrossentropy()\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss,\n",
    "    metrics = metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7514a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DistilBERT Model\n",
    "# train_history = model.fit(\n",
    "#     x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "#     y = y_train,\n",
    "#     validation_data = (\n",
    "#     {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "#     ),\n",
    "#     epochs=10,\n",
    "#     batch_size=64,\n",
    "# #     class_weight=class_weights_dict\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "951b01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERT Model\n",
    "# train_history = model.fit(\n",
    "#     x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "#     y = y_train,\n",
    "#     validation_data = (\n",
    "#     {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "#     ),\n",
    "#   epochs=3,\n",
    "#     batch_size=64\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5c08b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 21:00:58.061825: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f447cd31d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-18 21:00:58.061842: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-09-18 21:00:58.077839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-18 21:00:58.109846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-18 21:00:58.233595: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 116s 158ms/step - loss: 1.3020 - balanced_accuracy: 0.6663 - val_loss: 0.6533 - val_balanced_accuracy: 0.8229\n",
      "Epoch 2/4\n",
      "596/596 [==============================] - 82s 138ms/step - loss: 0.5390 - balanced_accuracy: 0.8502 - val_loss: 0.4962 - val_balanced_accuracy: 0.8564\n",
      "Epoch 3/4\n",
      "596/596 [==============================] - 82s 137ms/step - loss: 0.3577 - balanced_accuracy: 0.8971 - val_loss: 0.4461 - val_balanced_accuracy: 0.8706\n",
      "Epoch 4/4\n",
      "596/596 [==============================] - 82s 138ms/step - loss: 0.2615 - balanced_accuracy: 0.9232 - val_loss: 0.4422 - val_balanced_accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa Model\n",
    "train_history = model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "    y = y_train,\n",
    "    validation_data = (\n",
    "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "    ),\n",
    "  epochs=4,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c25407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "452/452 [==============================] - 102s 181ms/step - loss: 1.5646 - balanced_accuracy: 0.6058 - val_loss: 1.2900 - val_balanced_accuracy: 0.6781\n",
      "Epoch 2/4\n",
      "452/452 [==============================] - 73s 161ms/step - loss: 0.7842 - balanced_accuracy: 0.7924 - val_loss: 0.8829 - val_balanced_accuracy: 0.7837\n",
      "Epoch 3/4\n",
      "452/452 [==============================] - 71s 158ms/step - loss: 0.5435 - balanced_accuracy: 0.8551 - val_loss: 0.7161 - val_balanced_accuracy: 0.8214\n",
      "Epoch 4/4\n",
      "452/452 [==============================] - 71s 158ms/step - loss: 0.4025 - balanced_accuracy: 0.8897 - val_loss: 0.7366 - val_balanced_accuracy: 0.8268\n"
     ]
    }
   ],
   "source": [
    "# # XLNet Model\n",
    "# train_history = model.fit(\n",
    "#     x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "#     y = y_train,\n",
    "#     validation_data = (\n",
    "#     {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "#     ),\n",
    "#   epochs=10,\n",
    "#     batch_size=64\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91fd2172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/298 [==============================] - 9s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.1248064e-05, 3.8477272e-05, 6.4755768e-06, 5.3644297e-04,\n",
       "       1.5372982e-04, 5.4507726e-03, 8.6903796e-03, 3.9193914e-03,\n",
       "       2.4859102e-03, 9.1927785e-01, 6.3199503e-04, 2.1341922e-02,\n",
       "       1.7573903e-04, 2.8402400e-03, 7.6242577e-04, 2.8557093e-03,\n",
       "       5.4726956e-06, 1.0088938e-04, 3.0560743e-05, 1.7261207e-03,\n",
       "       4.1193925e-04, 1.1706063e-02, 3.3405636e-04, 1.3793874e-04,\n",
       "       1.5719777e-02, 7.7284218e-05, 1.5471120e-05, 1.9848807e-04,\n",
       "       3.0338063e-04, 5.3842774e-05], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\n",
    "predicted_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af24993",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.argmax(predicted_raw, axis = 1)\n",
    "y_true = df_test.event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60044245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DistilBERT\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4131a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERT\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53f0df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.99      0.99       137\n",
      "           2       0.94      0.96      0.95       159\n",
      "           3       0.99      0.97      0.98       302\n",
      "           4       0.97      0.96      0.97       303\n",
      "           5       0.99      0.95      0.97        97\n",
      "           6       0.73      0.70      0.72        88\n",
      "           7       0.77      0.84      0.80       411\n",
      "           8       0.76      0.89      0.82       131\n",
      "           9       0.82      0.81      0.81       447\n",
      "          10       1.00      1.00      1.00       154\n",
      "          11       0.79      0.80      0.79      1030\n",
      "          12       1.00      1.00      1.00       145\n",
      "          13       0.80      0.89      0.84       422\n",
      "          14       0.97      0.94      0.96       124\n",
      "          15       0.90      0.85      0.87      2110\n",
      "          16       0.87      0.76      0.81        80\n",
      "          17       1.00      1.00      1.00       106\n",
      "          18       1.00      1.00      1.00       116\n",
      "          19       0.65      0.78      0.71       158\n",
      "          20       0.57      0.36      0.45        85\n",
      "          21       0.68      0.55      0.61       107\n",
      "          22       0.97      0.95      0.96       114\n",
      "          23       0.97      0.99      0.98       340\n",
      "          24       0.84      0.85      0.84      1209\n",
      "          25       0.99      1.00      1.00       204\n",
      "          26       0.91      0.91      0.91       530\n",
      "          27       0.99      0.96      0.97        71\n",
      "          28       0.84      0.95      0.89       117\n",
      "          29       0.96      0.99      0.97       228\n",
      "\n",
      "    accuracy                           0.87      9525\n",
      "   macro avg       0.88      0.88      0.88      9525\n",
      "weighted avg       0.87      0.87      0.87      9525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85210fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XLNet\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8d0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2224cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a969d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
